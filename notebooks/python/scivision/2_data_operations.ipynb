{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fba2a3",
   "metadata": {},
   "source": [
    "# Data operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce454ac2",
   "metadata": {},
   "source": [
    "This notebook contains some code for loading the images and classification labels.\n",
    "\n",
    "**The [last cell of this notebook](#Quick-start) contains everything needed to load the labelled training data into an xarray, in a single notebook cell.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef455d06",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95331c93",
   "metadata": {
    "gather": {
     "logged": 1636726275244
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scivision.io import load_dataset\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab231d",
   "metadata": {},
   "source": [
    "## Load the Intake catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f4a299",
   "metadata": {},
   "source": [
    "As before, load the [Intake](https://intake.readthedocs.io/en/latest/index.html) catalog from the challenge repository containing [Scivision](https://github.com/alan-turing-institute/scivision) metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1beaacda",
   "metadata": {
    "gather": {
     "logged": 1636726275627
    }
   },
   "outputs": [],
   "source": [
    "cat = load_dataset('https://github.com/alan-turing-institute/plankton-dsg-challenge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d474ede6",
   "metadata": {},
   "source": [
    "## Inspect the catalog entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9713391",
   "metadata": {},
   "source": [
    "We explored the catalog in the previous notebook. It contains several data sources: their descriptions are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5dde03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>plankton_single</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Load a single labeled images from CEFAS zooplankton dataset"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>plankton_multiple</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Labeled images from CEFAS zooplankton dataset"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>labels_raw</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Contains the classification labels for all images. IMPORTANT NOTE: only use this data source if you intend to fetch the labels for ALL of the images (both the test and training set)\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>labels</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Contains a subset of filenames to use as the primary working dataset for the challenge"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>labels_holdout</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Contains a subset of filenames to be used as the final holdout set, for model assessment"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data_source in cat:\n",
    "    display(HTML(f\"<h4>{data_source}</h4>\"))\n",
    "    display(HTML(cat[data_source].description))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93e176",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">We will use the <tt>plankton_multiple</tt> entry to fetch all of the images, and the <tt>labels</tt> to fetch the labels for training.  The <tt>labels_holdout</tt> will be useful as a final holdout set for testing any models you may produce during the challenge.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabac06d",
   "metadata": {},
   "source": [
    "## Fetch the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ae618",
   "metadata": {},
   "source": [
    "The `labels` entry corresponds to an index file, imported as a `pandas.DataFrame`, which contain the list of all plankton images. Each image include its index, filename, and labels according to three levels of classication: `label1` (zooplankton vs detritus), `label2` (noncopedod vs copedod) and `label3` (species)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c17264",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels = cat.labels().read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9977f8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Pia1.2016-10-04.1801+N292_hc.tif</td>\n",
       "      <td>zooplankton</td>\n",
       "      <td>noncopepod</td>\n",
       "      <td>annelida_polychaeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pia1.2016-10-05.1229+N28_hc.tif</td>\n",
       "      <td>zooplankton</td>\n",
       "      <td>noncopepod</td>\n",
       "      <td>annelida_polychaeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Pia1.2016-10-06.2118+N136_hc.tif</td>\n",
       "      <td>zooplankton</td>\n",
       "      <td>noncopepod</td>\n",
       "      <td>annelida_polychaeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Pia1.2017-03-21.1136+N01644266_hc.tif</td>\n",
       "      <td>zooplankton</td>\n",
       "      <td>noncopepod</td>\n",
       "      <td>annelida_polychaeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Pia1.2017-03-21.1136+N01646706_hc.tif</td>\n",
       "      <td>zooplankton</td>\n",
       "      <td>noncopepod</td>\n",
       "      <td>annelida_polychaeta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                               filename       label1      label2  \\\n",
       "0      1       Pia1.2016-10-04.1801+N292_hc.tif  zooplankton  noncopepod   \n",
       "1      2        Pia1.2016-10-05.1229+N28_hc.tif  zooplankton  noncopepod   \n",
       "2      3       Pia1.2016-10-06.2118+N136_hc.tif  zooplankton  noncopepod   \n",
       "3      4  Pia1.2017-03-21.1136+N01644266_hc.tif  zooplankton  noncopepod   \n",
       "4      5  Pia1.2017-03-21.1136+N01646706_hc.tif  zooplankton  noncopepod   \n",
       "\n",
       "                label3  \n",
       "0  annelida_polychaeta  \n",
       "1  annelida_polychaeta  \n",
       "2  annelida_polychaeta  \n",
       "3  annelida_polychaeta  \n",
       "4  annelida_polychaeta  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c10750",
   "metadata": {},
   "source": [
    "## Fetch the complete image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844b2d60",
   "metadata": {},
   "source": [
    "The final entry refers to load the full dataset. All images are stacked into a single `xarray.Dataset` object with  fixed dimensions of 1040 px x 832, large enough to hold all of the images.  Smaller images are padded with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab08a3",
   "metadata": {
    "gather": {
     "logged": 1636726330314
    }
   },
   "outputs": [],
   "source": [
    "ds_all = cat.plankton_multiple().to_dask()\n",
    "ds_all.filename.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a5c6a",
   "metadata": {
    "gather": {
     "logged": 1636726330509
    }
   },
   "outputs": [],
   "source": [
    "print(ds_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d8cff",
   "metadata": {},
   "source": [
    "Let's subset a single image. This can be done using the image index stored in `concat_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad283301",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ds_all.sel(concat_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91591241",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570169ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(subset['raster'].compute().values[:,:,:])\n",
    "plt.title(subset.filename.compute().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d551d",
   "metadata": {},
   "source": [
    "## Assembling the labelled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64c882a",
   "metadata": {},
   "source": [
    "### Check for duplicate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, label_grp in labels.groupby(\"filename\"):\n",
    "    if len(label_grp) > 1:\n",
    "        display(label_grp.reset_index(drop=True))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb623b",
   "metadata": {},
   "source": [
    "### Join the images and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce2025d",
   "metadata": {},
   "source": [
    "Put the labels into an xarray, dropping any filenames that have duplicate labels. Set the filename as the (unique) index so it ready to be merged (joined) with the image data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ca007",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dedup = xr.Dataset.from_dataframe(\n",
    "    labels\n",
    "    .drop_duplicates(subset=[\"filename\"])\n",
    "    .set_index(\"filename\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "print(labels_dedup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f10cb7",
   "metadata": {},
   "source": [
    "Merging (joining) a dataset can be done on xarray dimensions. We temporarily make `filename` a dimension of the dataset in order to perform the merge (in place of `concat_dim` - the integer-valued dimension corresponding to each image file to read in ds_all, which is **not** the same as `index` in labels_dedup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_labelled = (\n",
    "    ds_all\n",
    "    .swap_dims({\"concat_dim\": \"filename\"})\n",
    "    .merge(labels_dedup, join=\"inner\")\n",
    "    .swap_dims({\"filename\": \"concat_dim\"})\n",
    ")\n",
    "\n",
    "print(ds_labelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f5726",
   "metadata": {},
   "source": [
    "## Quick start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45345c9",
   "metadata": {},
   "source": [
    "The following cell contains everything needed to load the labelled training data into an xarray, named `ds_labelled` (independent of the rest of the notebook). It will take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e10bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from scivision.io import load_dataset\n",
    "\n",
    "cat = load_dataset('https://github.com/alan-turing-institute/plankton-dsg-challenge')\n",
    "\n",
    "ds_all = cat.plankton_multiple().to_dask()\n",
    "labels = cat.labels().read()\n",
    "\n",
    "labels_dedup = xr.Dataset.from_dataframe(\n",
    "    labels\n",
    "    .drop_duplicates(subset=[\"filename\"])\n",
    "    .set_index(\"filename\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "ds_labelled = (\n",
    "    ds_all\n",
    "    .swap_dims({\"concat_dim\": \"filename\"})\n",
    "    .merge(labels_dedup, join=\"inner\")\n",
    "    .swap_dims({\"filename\": \"concat_dim\"})\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "scivis-plankton"
  },
  "kernelspec": {
   "display_name": "Python (scivis-plankton)",
   "language": "python",
   "name": "scivis-plankton"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
