{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check for image overlaps in two classes\n",
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "def preprocess(im, size, blur, gray=True):\n",
    "#     im = cv2.GaussianBlur(im, (blur, blur),0)\n",
    "    if gray:\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    im = cv2.resize(im,(size, size))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Pia1.2016-10-04.1801+N292_hc.tif</td>\n",
       "      <td>zooplankton</td>\n",
       "      <td>noncopepod</td>\n",
       "      <td>annelida_polychaeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pia1.2016-10-05.1229+N28_hc.tif</td>\n",
       "      <td>zooplankton</td>\n",
       "      <td>noncopepod</td>\n",
       "      <td>annelida_polychaeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Pia1.2016-10-06.2118+N136_hc.tif</td>\n",
       "      <td>zooplankton</td>\n",
       "      <td>noncopepod</td>\n",
       "      <td>annelida_polychaeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Pia1.2017-03-21.1136+N01644266_hc.tif</td>\n",
       "      <td>zooplankton</td>\n",
       "      <td>noncopepod</td>\n",
       "      <td>annelida_polychaeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Pia1.2017-03-21.1136+N01646706_hc.tif</td>\n",
       "      <td>zooplankton</td>\n",
       "      <td>noncopepod</td>\n",
       "      <td>annelida_polychaeta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                               filename       label1      label2  \\\n",
       "0      1       Pia1.2016-10-04.1801+N292_hc.tif  zooplankton  noncopepod   \n",
       "1      2        Pia1.2016-10-05.1229+N28_hc.tif  zooplankton  noncopepod   \n",
       "2      3       Pia1.2016-10-06.2118+N136_hc.tif  zooplankton  noncopepod   \n",
       "3      4  Pia1.2017-03-21.1136+N01644266_hc.tif  zooplankton  noncopepod   \n",
       "4      5  Pia1.2017-03-21.1136+N01646706_hc.tif  zooplankton  noncopepod   \n",
       "\n",
       "                label3  \n",
       "0  annelida_polychaeta  \n",
       "1  annelida_polychaeta  \n",
       "2  annelida_polychaeta  \n",
       "3  annelida_polychaeta  \n",
       "4  annelida_polychaeta  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set VM data path and read labels file\n",
    "\n",
    "dataPath = '/scratch/data/images/'\n",
    "df_ind = pd.read_csv('../../../data/processed/test-train/train.csv')\n",
    "df_ind.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Images to Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "detritus      36000\n",
       "copepod        9253\n",
       "noncopepod     6056\n",
       "Name: label2, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ind['label2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9253 6056\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "cop_im_list = df_ind[df_ind['label2']=='copepod']['filename'].tolist()\n",
    "noncop_im_list = df_ind[df_ind['label2']=='noncopepod']['filename'].tolist()\n",
    "\n",
    "print(len(cop_im_list), len(noncop_im_list))\n",
    "print(intersection(cop_im_list, noncop_im_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9254\n"
     ]
    }
   ],
   "source": [
    "savePath = '/shared/jenniferding/plankton_c/all_cop/all_cop/'\n",
    "augmentPath = savePath + \"augment/\"\n",
    "\n",
    "n = len(cop_im_list)\n",
    "\n",
    "for i in sorted(cop_im_list)[:n]:\n",
    "    im = cv2.imread(dataPath + i)\n",
    "    im = preprocess(im, 64, 3, gray=True)\n",
    "    savename = i.replace('.tif', '.png')\n",
    "    cv2.imwrite(savePath+savename, im)\n",
    "    \n",
    "print(len(os.listdir(savePath)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://keras.io/examples/generative/dcgan_overriding_train_step/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9253 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/shared/jenniferding/plankton_c/all_cop/\", label_mode=None, image_size=(64,64), batch_size=32\n",
    ")\n",
    "dataset = dataset.map(lambda x: x / 255.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x in dataset:\n",
    "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 404,801\n",
      "Trainable params: 404,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(64,64,3)),\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8192)              1056768   \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 3)         38403     \n",
      "=================================================================\n",
      "Total params: 3,979,651\n",
      "Trainable params: 3,979,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(8 * 8 * 128),\n",
    "        layers.Reshape((8, 8, 128)),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            img.save(augmentPath+\"generated_img_%03d_%d.png\" % (epoch, i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "290/290 [==============================] - 1286s 4s/step - d_loss: 0.6924 - g_loss: 1.0399\n",
      "Epoch 2/50\n",
      "290/290 [==============================] - 1276s 4s/step - d_loss: 0.5920 - g_loss: 1.1322\n",
      "Epoch 3/50\n",
      "290/290 [==============================] - 1306s 4s/step - d_loss: 0.6420 - g_loss: 1.0041\n",
      "Epoch 4/50\n",
      "290/290 [==============================] - 1282s 4s/step - d_loss: 0.5844 - g_loss: 1.2805\n",
      "Epoch 5/50\n",
      "290/290 [==============================] - 1299s 4s/step - d_loss: 0.6442 - g_loss: 0.9805\n",
      "Epoch 6/50\n",
      "290/290 [==============================] - 1297s 4s/step - d_loss: 0.6842 - g_loss: 1.4341\n",
      "Epoch 7/50\n",
      "290/290 [==============================] - 1302s 4s/step - d_loss: 0.5824 - g_loss: 1.4492\n",
      "Epoch 8/50\n",
      "290/290 [==============================] - 1301s 4s/step - d_loss: 0.7683 - g_loss: 0.8417\n",
      "Epoch 9/50\n",
      "290/290 [==============================] - 1264s 4s/step - d_loss: 0.6757 - g_loss: 0.9589\n",
      "Epoch 10/50\n",
      "290/290 [==============================] - 1265s 4s/step - d_loss: 0.7372 - g_loss: 0.7942\n",
      "Epoch 11/50\n",
      "290/290 [==============================] - 1277s 4s/step - d_loss: 0.6956 - g_loss: 0.8319\n",
      "Epoch 12/50\n",
      "290/290 [==============================] - 1271s 4s/step - d_loss: 0.7063 - g_loss: 0.8401\n",
      "Epoch 13/50\n",
      "290/290 [==============================] - 1276s 4s/step - d_loss: 0.7319 - g_loss: 0.7843\n",
      "Epoch 14/50\n",
      "290/290 [==============================] - 1266s 4s/step - d_loss: 0.6982 - g_loss: 0.8387\n",
      "Epoch 15/50\n",
      "290/290 [==============================] - 1547s 5s/step - d_loss: 0.6968 - g_loss: 0.8183\n",
      "Epoch 16/50\n",
      "290/290 [==============================] - 1824s 6s/step - d_loss: 0.7035 - g_loss: 0.8070\n",
      "Epoch 17/50\n",
      "290/290 [==============================] - 1842s 6s/step - d_loss: 0.7114 - g_loss: 0.8453\n",
      "Epoch 18/50\n",
      "290/290 [==============================] - 1859s 6s/step - d_loss: 0.7126 - g_loss: 0.8420\n",
      "Epoch 19/50\n",
      "290/290 [==============================] - 1844s 6s/step - d_loss: 0.7079 - g_loss: 0.8491\n",
      "Epoch 20/50\n",
      "290/290 [==============================] - 1837s 6s/step - d_loss: 0.6964 - g_loss: 0.8903\n",
      "Epoch 21/50\n",
      "290/290 [==============================] - 1804s 6s/step - d_loss: 0.7046 - g_loss: 0.8229\n",
      "Epoch 22/50\n",
      "290/290 [==============================] - 1809s 6s/step - d_loss: 0.6824 - g_loss: 0.8507\n",
      "Epoch 23/50\n",
      "290/290 [==============================] - 1774s 6s/step - d_loss: 0.6832 - g_loss: 0.8138\n",
      "Epoch 24/50\n",
      "290/290 [==============================] - 1627s 6s/step - d_loss: 0.6964 - g_loss: 0.7856\n",
      "Epoch 25/50\n",
      "290/290 [==============================] - 1852s 6s/step - d_loss: 0.6912 - g_loss: 0.8051\n",
      "Epoch 26/50\n",
      "290/290 [==============================] - 1851s 6s/step - d_loss: 0.6843 - g_loss: 0.9450\n",
      "Epoch 27/50\n",
      "290/290 [==============================] - 1878s 6s/step - d_loss: 0.7042 - g_loss: 0.8262\n",
      "Epoch 28/50\n",
      "290/290 [==============================] - 1897s 7s/step - d_loss: 0.7118 - g_loss: 0.8259\n",
      "Epoch 29/50\n",
      "290/290 [==============================] - 1902s 7s/step - d_loss: 0.7139 - g_loss: 0.9190\n",
      "Epoch 30/50\n",
      "290/290 [==============================] - 1900s 7s/step - d_loss: 0.7072 - g_loss: 0.7907\n",
      "Epoch 31/50\n",
      "290/290 [==============================] - 1884s 6s/step - d_loss: 0.7203 - g_loss: 0.7919\n",
      "Epoch 32/50\n",
      "290/290 [==============================] - 1894s 7s/step - d_loss: 0.6889 - g_loss: 0.8899\n",
      "Epoch 33/50\n",
      "290/290 [==============================] - 1878s 6s/step - d_loss: 0.7272 - g_loss: 0.8459\n",
      "Epoch 34/50\n",
      "290/290 [==============================] - 1873s 6s/step - d_loss: 0.6980 - g_loss: 0.8611\n",
      "Epoch 35/50\n",
      "290/290 [==============================] - 1873s 6s/step - d_loss: 0.6766 - g_loss: 0.8127\n",
      "Epoch 36/50\n",
      "290/290 [==============================] - 1881s 6s/step - d_loss: 0.6863 - g_loss: 0.8265\n",
      "Epoch 37/50\n",
      "290/290 [==============================] - 1666s 6s/step - d_loss: 0.6961 - g_loss: 0.8346\n",
      "Epoch 38/50\n",
      "290/290 [==============================] - 1367s 5s/step - d_loss: 0.7119 - g_loss: 0.7961\n",
      "Epoch 39/50\n",
      "290/290 [==============================] - 1364s 5s/step - d_loss: 0.7136 - g_loss: 0.8146\n",
      "Epoch 40/50\n",
      "290/290 [==============================] - 1542s 5s/step - d_loss: 0.7282 - g_loss: 0.8443\n",
      "Epoch 41/50\n",
      "290/290 [==============================] - 1304s 4s/step - d_loss: 0.7067 - g_loss: 0.7852\n",
      "Epoch 42/50\n",
      "290/290 [==============================] - 1294s 4s/step - d_loss: 0.7009 - g_loss: 0.7558\n",
      "Epoch 43/50\n",
      "290/290 [==============================] - 1310s 5s/step - d_loss: 0.7131 - g_loss: 0.8607\n",
      "Epoch 44/50\n",
      "290/290 [==============================] - 1339s 5s/step - d_loss: 0.6995 - g_loss: 1.1289\n",
      "Epoch 45/50\n",
      "290/290 [==============================] - 1300s 4s/step - d_loss: 0.6918 - g_loss: 0.7876\n",
      "Epoch 46/50\n",
      "290/290 [==============================] - 1475s 5s/step - d_loss: 0.7117 - g_loss: 0.8730\n",
      "Epoch 47/50\n",
      "290/290 [==============================] - 1842s 6s/step - d_loss: 0.6744 - g_loss: 0.8522\n",
      "Epoch 48/50\n",
      "290/290 [==============================] - 1724s 6s/step - d_loss: 0.6709 - g_loss: 0.9558\n",
      "Epoch 49/50\n",
      "290/290 [==============================] - 1813s 6s/step - d_loss: 0.7123 - g_loss: 0.8590\n",
      "Epoch 50/50\n",
      "290/290 [==============================] - 1809s 6s/step - d_loss: 0.7174 - g_loss: 0.8437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdccc0e99a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50  # In practice, use ~100 epochs\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=5, latent_dim=latent_dim)]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scivis-plankton)",
   "language": "python",
   "name": "scivis-plankton"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
