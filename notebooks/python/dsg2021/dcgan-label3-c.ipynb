{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check for image overlaps in two classes\n",
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "def highpass(im, sigma):\n",
    "    return im - cv2.GaussianBlur(im, (0,0), sigma) + 127\n",
    "\n",
    "def preprocess(im, size, blur, gray=True):\n",
    "    im = cv2.GaussianBlur(im, (blur, blur),0)\n",
    "#     im = highpass(im, blur)\n",
    "    if gray:\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    im = cv2.resize(im,(size, size))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Pia1.2016-10-04.1801+N292_hc.tif</td>\n",
       "      <td>zooplankton</td>\n",
       "      <td>noncopepod</td>\n",
       "      <td>annelida_polychaeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pia1.2016-10-05.1229+N28_hc.tif</td>\n",
       "      <td>zooplankton</td>\n",
       "      <td>noncopepod</td>\n",
       "      <td>annelida_polychaeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Pia1.2016-10-06.2118+N136_hc.tif</td>\n",
       "      <td>zooplankton</td>\n",
       "      <td>noncopepod</td>\n",
       "      <td>annelida_polychaeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Pia1.2017-03-21.1136+N01644266_hc.tif</td>\n",
       "      <td>zooplankton</td>\n",
       "      <td>noncopepod</td>\n",
       "      <td>annelida_polychaeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Pia1.2017-03-21.1136+N01646706_hc.tif</td>\n",
       "      <td>zooplankton</td>\n",
       "      <td>noncopepod</td>\n",
       "      <td>annelida_polychaeta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                               filename       label1      label2  \\\n",
       "0      1       Pia1.2016-10-04.1801+N292_hc.tif  zooplankton  noncopepod   \n",
       "1      2        Pia1.2016-10-05.1229+N28_hc.tif  zooplankton  noncopepod   \n",
       "2      3       Pia1.2016-10-06.2118+N136_hc.tif  zooplankton  noncopepod   \n",
       "3      4  Pia1.2017-03-21.1136+N01644266_hc.tif  zooplankton  noncopepod   \n",
       "4      5  Pia1.2017-03-21.1136+N01646706_hc.tif  zooplankton  noncopepod   \n",
       "\n",
       "                label3  \n",
       "0  annelida_polychaeta  \n",
       "1  annelida_polychaeta  \n",
       "2  annelida_polychaeta  \n",
       "3  annelida_polychaeta  \n",
       "4  annelida_polychaeta  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set VM data path and read labels file\n",
    "\n",
    "# dataPath = '/scratch/data/images/'\n",
    "dataPath = '/output/data/augment/copepod_calanoida_acartia-spp/'\n",
    "df_ind = pd.read_csv('../../../data/processed/test-train/train.csv')\n",
    "df_ind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "copepod_calanoida_para-pseudocalanus-spp    1790\n",
       "copepod_unknown                             1648\n",
       "copepod_calanoida                           1491\n",
       "copepod_cyclopoida_corycaeus-spp             999\n",
       "copepod_calanoida_centropages-spp            692\n",
       "copepod_cyclopoida_oncaea-spp                630\n",
       "copepod_harpacticoida                        575\n",
       "copepod_cyclopoida_oithona-spp               441\n",
       "copepod_calanoida_acartia-spp                405\n",
       "copepod_calanoida_calanus-spp                311\n",
       "copepod_calanoida_temora-spp                 147\n",
       "copepod_cyclopoida                            88\n",
       "copepod_calanoida_candacia-spp                36\n",
       "Name: label3, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ind[df_ind['label2']=='copepod']['label3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['copepod_unknown',\n",
       " 'copepod_calanoida',\n",
       " 'copepod_calanoida_acartia-spp',\n",
       " 'copepod_calanoida_calanus-spp',\n",
       " 'copepod_calanoida_candacia-spp',\n",
       " 'copepod_calanoida_centropages-spp',\n",
       " 'copepod_calanoida_para-pseudocalanus-spp',\n",
       " 'copepod_calanoida_temora-spp',\n",
       " 'copepod_cyclopoida',\n",
       " 'copepod_cyclopoida_corycaeus-spp',\n",
       " 'copepod_cyclopoida_oithona-spp',\n",
       " 'copepod_cyclopoida_oncaea-spp',\n",
       " 'copepod_harpacticoida']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ind[df_ind['label2']=='copepod']['label3'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Images to Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"copepod_calanoida_acartia-spp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2430\n"
     ]
    }
   ],
   "source": [
    "# label_im_list = df_ind[df_ind['label3']==label]['filename'].tolist()\n",
    "\n",
    "label_im_list = os.listdir(dataPath)\n",
    "\n",
    "print(len(label_im_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2431\n"
     ]
    }
   ],
   "source": [
    "savePath = '/shared/jenniferding/plankton_c/' + str(label) + '/' + str(label) + '/'\n",
    "augmentPath = savePath + \"augment/\"\n",
    "\n",
    "if not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)\n",
    "    os.makedirs(augmentPath)\n",
    "\n",
    "for i in sorted(label_im_list):\n",
    "    im = cv2.imread(dataPath + i)\n",
    "    im = preprocess(im, 64, 3, gray=True)\n",
    "    savename = i.replace('.tif', '.png')\n",
    "    cv2.imwrite(savePath+savename, im)\n",
    "    \n",
    "print(len(os.listdir(savePath)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://keras.io/examples/generative/dcgan_overriding_train_step/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2430 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/shared/jenniferding/plankton_c/\" + str(label), label_mode=None, image_size=(64,64), batch_size=32\n",
    ")\n",
    "dataset = dataset.map(lambda x: x / 255.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x in dataset:\n",
    "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 404,801\n",
      "Trainable params: 404,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(64, 64, 3)),\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8192)              1056768   \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 3)         38403     \n",
      "=================================================================\n",
      "Total params: 3,979,651\n",
      "Trainable params: 3,979,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(8 * 8 * 128),\n",
    "        layers.Reshape((8, 8, 128)),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            img.save(augmentPath+\"generated_img_%03d_%d.png\" % (epoch, i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "76/76 [==============================] - 481s 6s/step - d_loss: 0.5166 - g_loss: 0.9416\n",
      "Epoch 2/50\n",
      "76/76 [==============================] - 486s 6s/step - d_loss: 1.0009 - g_loss: 0.7746\n",
      "Epoch 3/50\n",
      "76/76 [==============================] - 483s 6s/step - d_loss: 0.4396 - g_loss: 1.5045\n",
      "Epoch 4/50\n",
      "76/76 [==============================] - 486s 6s/step - d_loss: 0.5257 - g_loss: 1.1118\n",
      "Epoch 5/50\n",
      "76/76 [==============================] - 488s 6s/step - d_loss: 0.4448 - g_loss: 1.6695\n",
      "Epoch 6/50\n",
      "76/76 [==============================] - 487s 6s/step - d_loss: 0.5438 - g_loss: 1.5660\n",
      "Epoch 7/50\n",
      "76/76 [==============================] - 482s 6s/step - d_loss: 0.6612 - g_loss: 1.8310\n",
      "Epoch 8/50\n",
      "76/76 [==============================] - 488s 6s/step - d_loss: 0.5358 - g_loss: 4.2111\n",
      "Epoch 9/50\n",
      "76/76 [==============================] - 486s 6s/step - d_loss: 0.7564 - g_loss: 0.8573\n",
      "Epoch 10/50\n",
      "76/76 [==============================] - 487s 6s/step - d_loss: 0.6750 - g_loss: 0.9380\n",
      "Epoch 11/50\n",
      "76/76 [==============================] - 492s 6s/step - d_loss: 0.7270 - g_loss: 0.8946\n",
      "Epoch 12/50\n",
      "76/76 [==============================] - 492s 6s/step - d_loss: 0.5781 - g_loss: 1.1421\n",
      "Epoch 13/50\n",
      "76/76 [==============================] - 495s 6s/step - d_loss: 0.6340 - g_loss: 1.1468\n",
      "Epoch 14/50\n",
      "76/76 [==============================] - 494s 6s/step - d_loss: 0.3673 - g_loss: 1.6379\n",
      "Epoch 15/50\n",
      "76/76 [==============================] - 497s 7s/step - d_loss: 0.7466 - g_loss: 1.0167\n",
      "Epoch 16/50\n",
      "76/76 [==============================] - 497s 7s/step - d_loss: 0.6134 - g_loss: 1.0933\n",
      "Epoch 17/50\n",
      "76/76 [==============================] - 501s 7s/step - d_loss: 0.7186 - g_loss: 1.2226\n",
      "Epoch 18/50\n",
      "76/76 [==============================] - 502s 7s/step - d_loss: 0.2101 - g_loss: 3.3370\n",
      "Epoch 19/50\n",
      "76/76 [==============================] - 507s 7s/step - d_loss: 0.8747 - g_loss: 0.9518\n",
      "Epoch 20/50\n",
      "76/76 [==============================] - 498s 7s/step - d_loss: 0.6847 - g_loss: 1.0335\n",
      "Epoch 21/50\n",
      "76/76 [==============================] - 501s 7s/step - d_loss: 0.3839 - g_loss: 1.4162\n",
      "Epoch 22/50\n",
      "76/76 [==============================] - 501s 7s/step - d_loss: 0.1752 - g_loss: 2.3865\n",
      "Epoch 23/50\n",
      "76/76 [==============================] - 504s 7s/step - d_loss: 0.2906 - g_loss: 2.7913\n",
      "Epoch 24/50\n",
      "76/76 [==============================] - 496s 7s/step - d_loss: 0.3095 - g_loss: 3.1507\n",
      "Epoch 25/50\n",
      "76/76 [==============================] - 489s 6s/step - d_loss: 0.7831 - g_loss: 1.0900\n",
      "Epoch 26/50\n",
      "76/76 [==============================] - 486s 6s/step - d_loss: 0.6169 - g_loss: 2.4362\n",
      "Epoch 27/50\n",
      "76/76 [==============================] - 488s 6s/step - d_loss: 0.6523 - g_loss: 0.9598\n",
      "Epoch 28/50\n",
      "76/76 [==============================] - 505s 7s/step - d_loss: 0.2930 - g_loss: 2.0183\n",
      "Epoch 29/50\n",
      "76/76 [==============================] - 499s 7s/step - d_loss: 0.4487 - g_loss: 1.6942\n",
      "Epoch 30/50\n",
      "76/76 [==============================] - 502s 7s/step - d_loss: 0.4408 - g_loss: 1.3695\n",
      "Epoch 31/50\n",
      "76/76 [==============================] - 499s 7s/step - d_loss: 0.5546 - g_loss: 1.2701\n",
      "Epoch 32/50\n",
      "76/76 [==============================] - 495s 6s/step - d_loss: 0.5926 - g_loss: 1.0812\n",
      "Epoch 33/50\n",
      "76/76 [==============================] - 498s 7s/step - d_loss: 0.6194 - g_loss: 0.9102\n",
      "Epoch 34/50\n",
      "76/76 [==============================] - 494s 6s/step - d_loss: 0.6494 - g_loss: 0.9169\n",
      "Epoch 35/50\n",
      "76/76 [==============================] - 493s 6s/step - d_loss: 0.7647 - g_loss: 0.7814\n",
      "Epoch 36/50\n",
      "76/76 [==============================] - 492s 6s/step - d_loss: 0.6131 - g_loss: 0.9675\n",
      "Epoch 37/50\n",
      "76/76 [==============================] - 490s 6s/step - d_loss: 0.7167 - g_loss: 0.8165\n",
      "Epoch 38/50\n",
      "76/76 [==============================] - 490s 6s/step - d_loss: 0.6596 - g_loss: 0.9083\n",
      "Epoch 39/50\n",
      "76/76 [==============================] - 489s 6s/step - d_loss: 0.5370 - g_loss: 1.2373\n",
      "Epoch 40/50\n",
      "76/76 [==============================] - 487s 6s/step - d_loss: 0.7093 - g_loss: 0.9627\n",
      "Epoch 41/50\n",
      "76/76 [==============================] - 494s 6s/step - d_loss: 0.6767 - g_loss: 0.8731\n",
      "Epoch 42/50\n",
      "76/76 [==============================] - 494s 6s/step - d_loss: 0.6586 - g_loss: 0.8874\n",
      "Epoch 43/50\n",
      "76/76 [==============================] - 493s 6s/step - d_loss: 0.6904 - g_loss: 0.7862\n",
      "Epoch 44/50\n",
      "76/76 [==============================] - 494s 6s/step - d_loss: 0.7411 - g_loss: 0.7218\n",
      "Epoch 45/50\n",
      "76/76 [==============================] - 496s 6s/step - d_loss: 0.6595 - g_loss: 0.9350\n",
      "Epoch 46/50\n",
      "76/76 [==============================] - 490s 6s/step - d_loss: 0.7341 - g_loss: 0.8287\n",
      "Epoch 47/50\n",
      "76/76 [==============================] - 489s 6s/step - d_loss: 0.7124 - g_loss: 0.7963\n",
      "Epoch 48/50\n",
      "76/76 [==============================] - 499s 7s/step - d_loss: 0.7288 - g_loss: 0.7608\n",
      "Epoch 49/50\n",
      "76/76 [==============================] - 505s 7s/step - d_loss: 0.6518 - g_loss: 0.8881\n",
      "Epoch 50/50\n",
      "76/76 [==============================] - 508s 7s/step - d_loss: 0.7379 - g_loss: 0.7346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd76c27c220>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50  # In practice, use ~100 epochs\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=5, latent_dim=latent_dim)]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scivis-plankton)",
   "language": "python",
   "name": "scivis-plankton"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
